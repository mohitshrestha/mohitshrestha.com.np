[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Mohit Shrestha, MSBA",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     GitHub\n  \n  \n    \n     Twitter\n  \n  \n    \n     Email\n  \n\n  \n  \nHi there!\nCurrently, I am working as a Data Scientist Consultant at Data Elevates. I am well versed in R, Python, SAS, SQL, Tableau, and Power BI and specializes in data modeling and wrangling, building ML/AI models, analyzing and interpreting model results, and presenting impactful data insights to drive successful business solutions.\nI am motivated to support organizations striving to be more data-driven in their missions and solve problems by converting insights from data into actionable solutions.\nRecent experience\n\n\nChalkboard user\nData Scientist Consultant, Feb 2022 - present\n\n\nData Elevates\n\n\nBolt\nSenior Energy Market Analyst, May 2021 - Dec 2021\n\n\nEnergy Exemplar\n\n\nBolt\nEnergy Market Consultant, Jun 2011 - Sep 2016\n\n\nICF\nEducation\n\n\nGraduation Cap\nM.S. in Business Analytics | Sept 2019 - May 2021\n\n\nWake Forest University School of Business Winston-Salem, North Carolina, USA\n\n\nGraduation Cap\nB.S. in Mathematical Economics and Applied Mathematics | Aug 2007 - May 2011\n\n\nHampden-Sydney College Hampden Sydney, Virginia, USA"
  },
  {
    "objectID": "footer.html",
    "href": "footer.html",
    "title": "Mohit Shrestha",
    "section": "",
    "text": "Stay in touch\n\n\n\n\nIf you enjoyed my work, then don’t miss out on any future updates by subscribing to my email newsletter.\n\n\nSupport my work with a coffee\n\n\nOr if you’re interested in working with me, I’m open to freelance work. You can book an appointment with me on Calendly.\n\nShare\n\nTweet"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mohit Shrestha, MSBA",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     GitHub\n  \n  \n    \n     Twitter\n  \n  \n    \n     Email\n  \n\n  \n  \nHi there!\nCurrently, I am working as a Data Scientist Consultant at Data Elevates. I am well versed in R, Python, SAS, SQL, Tableau, and Power BI and specializes in data modeling and wrangling, building ML/AI models, analyzing and interpreting model results, and presenting impactful data insights to drive successful business solutions.\nI am motivated to support organizations striving to be more data-driven in their missions and solve problems by converting insights from data into actionable solutions.\nRecent experience\n\n\nChalkboard user\nData Scientist Consultant, Feb 2022 - present\n\n\nData Elevates\n\n\nBolt\nSenior Energy Market Analyst, May 2021 - Dec 2021\n\n\nEnergy Exemplar\n\n\nBolt\nEnergy Market Consultant, Jun 2011 - Sep 2016\n\n\nICF\nEducation\n\n\nGraduation Cap\nM.S. in Business Analytics | Sept 2019 - May 2021\n\n\nWake Forest University School of Business Winston-Salem, North Carolina, USA\n\n\nGraduation Cap\nB.S. in Mathematical Economics and Applied Mathematics | Aug 2007 - May 2011\n\n\nHampden-Sydney College Hampden Sydney, Virginia, USA\n\n\n\n\n\nServices\n\n\n\nDATA ANALYTICS\nExpert at data wrangling, ETL, and latest analytical tools to conduct through quantitative analysis.\n\n\nVISUALIZATIONS\nDesign and develop compelling data visualizations via a variety of techniques such as Tableau, R, Excel.\n\n\nMACHINE LEARNING\nResearch and implement appropriate Machine Learning algorithms and tools to discover solutions hidden in large data sets.\n\n\n\n\n\n\n\n\n\n\nBUSINESS INTELLIGENCE\nTranslate data into actionable business insights to drive business values and find business solutions.\n\n\nMARKET RESEARCH\nGather and organize information on target markets uncovering key insights.\n\n\nSTORYTELLER/ COMMUNICATOR\nMeasure, visualize, and communicate complex analytics to technical and nontechnical audiences with clarity, precision, and influence through a compelling narrative.\n\n\n\n\n\n\n\nPhone+1 (443) 823-4701\n\n\nEmailcontact@mohitshrestha.com.np\n\n\n\n\nInterested in my services? Hire me!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Driven Problem Solver\nEnergy Market Expert\n\n\n\n\n\n\nStay in touch\n\n\n\n\nIf you enjoyed my work, then don’t miss out on any future updates by subscribing to my email newsletter.\n\n\nSupport my work with a coffee\n\nOr if you’re interested in working with me, I’m open to freelance work. You can book an appointment with me on Calendly.\n\nShare\n\nTweet"
  },
  {
    "objectID": "posts/2019-05-29-everybody-has-to-start-somewhere/index.html",
    "href": "posts/2019-05-29-everybody-has-to-start-somewhere/index.html",
    "title": "Everybody has to start somewhere",
    "section": "",
    "text": "Image Source: https://www.callcentrehelper.com/images/stories/2018/01/boy-rocket-wall-cloudy-760.jpg\n\n\n\n“Everybody has to start somewhere. You have your whole future ahead of you. Perfection doesn’t happen right away.” ~ Haruki Murakami\n\nEverybody has to start somewhere, and this is my start. A start to my master’s degree in Business Analytics, a start to blogging, a start to a new learning journey. I am very excited as I have been wanting to do this for a while, and I’m nervous as well because it has been eight years since I graduated and going back to being a student is a bit intimidating. I also don’t have a strong coding and programming background, so this makes it more challenging for me. But this is what I want to do, and I will give my best to get the most out of this program.\nI have been in a similar position in the past. I double majored in Mathematical Economics and Applied Mathematics for my undergraduate degree, but right after graduation, I joined energy consulting. I jumped into a completely different industry than what I studied, so I had to do a lot of extra work just to understand the fundamentals of the electric/energy market. It was a steep learning curve, especially the first few months, but once I learned the basics, I was able to succeed in my job. I think I am in a similar situation now, where I know I will have to put in extra effort to learn the programming language, but I know this will eventually help me in my future courses, and in my career in the long term.\nI can only imagine the hard work, perseverance and sacrifice that is required to complete this online degree. Already in the first week, it was challenging to complete my assignments amidst my high school reunion that me and my friends had planned for months, before I enrolled in this program. But I knew I had to get it done, not because I had to submit the assignment to my professor, but because I have made a promise to myself to learn and do my best.\n\n\n\nImage Source: https://www.smartdatacollective.com/wp-content/uploads/2018/08/business-intelligence-big-data-768x512.jpg\n\n\nI am looking forward to this learning journey that I have embarked upon, and see how far I will reach from this very first ever blog post ever.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@misc{shrestha2019,\n  author = {Mohit Shrestha},\n  title = {Everybody Has to Start Somewhere},\n  date = {2019-05-29},\n  url = {https://www.mohitshrestha.com.np/posts/2019-05-29-everybody-has-to-start-somewhere},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMohit Shrestha. 2019. “Everybody Has to Start Somewhere.”\nMay 29, 2019. https://www.mohitshrestha.com.np/posts/2019-05-29-everybody-has-to-start-somewhere."
  },
  {
    "objectID": "posts/2019-06-03-from-zero-to-270-lines-of-sas-coding-and-lessons-learned/index.html",
    "href": "posts/2019-06-03-from-zero-to-270-lines-of-sas-coding-and-lessons-learned/index.html",
    "title": "From zero to 270 lines of SAS coding and lessons learned",
    "section": "",
    "text": "It’s my second week of learning to program in SAS. This week, we were introduced to the process of Data Wrangling. Data Wrangling is the process of acquiring, shaping transforming and cleansing data for analysis. The image below serves as a good guideline of the analytical process to follow in creating models for solving almost all business problems in an organized manner.\n\n\n\nFigure 1: Design Pattern\n\n\nThe assignment for this week was to follow these process to code a program in SAS to perform Portfolio Analysis. I am enjoying the format of learning to code in SAS and at the same time being introduced to basic financial analytical metrics such as Sharpe Ratio used by technical traders in the real world. The Sharpe ratio is a measure of the excess return relative to the variability of the portfolio return and is calculated by:\nSharpe Ratio = (Portfolio Expected Return – Risk Free rate) / Standard Deviation of Portfolio Return\nI was fascinated to learn about this simple formula, which helps investors in evaluating individual assets or diversified portfolios in order to maximize risk-adjusted returns. In just two weeks, with some guidelines from our Professor, I have moved on from knowing zero SAS code to learning to code 270 lines of SAS program to complete the portfolio analysis.\nAt this moment, I would like to make note of key items for future references, that I found little different/strange with learning SAS language.\n\nWhile Appending or Merging different structured data sets, SAS looks at all of the possible columns that are available in the output combined data set. If it doesn’t find or match the columns, then in the combined data set, it will initialize missing values for unmatched values in those columns.\nIn order to solve this missing values being initialized; we have to rename on import statement using Rename Option. SAS has the ability to Join and Match Merge data assuming they have a common key.\nSAS by default does the non-match, if we don’t specify conditional logic to control the outputting. So, SAS will do a full Outer Join by default, if you are familiar with SQL.\nTo do Match Merge in SAS, first we have to pre-sort the data by the variables that we want to join by. Typically, we don’t want to use descending order because if we do use, then in the by statement also we have to specify descending order. Once sorted, we merge the data with DATA step with the By statement.\n\n\n\n\nFigure 2: Join & Match Merge Process\n\n\n\nIn SAS basically the columns that we are grouping or joining by have to be the same name. So, SAS doesn’t have the name aliasing that we do with SQL. Like in SQL, I could have joined custID = ID from another table. We can’t do that with Data step in SAS, we have to use the RENAME options.\nRENAME option syntax is one of those weird SAS syntax thing. In almost every other programming language in the world including SAS, typically in formulas, the thing we are assigning is on the left side of the equal bar. For example if we are renaming region to be country, in SAS we would code it as RENAME=(REGION=COUNTRY). Also. if we had another column to rename then it would be just space, no commas needed. Just type space and old name = new name.\n\n\n\n\nFigure 3: RENAME Options Syntax\n\n\n\nIF THEN DO statement doesn’t work in typical procedures steps, so in those cases, we want to use Where Clause. Below image provides the general guideline for Where vs Sub-setting IF.\n\n\n\n\nFigure 4: Where VS Sub-setting IF\n\n\n\nSAS reads data sets row by row. N is an internal counter that allows us to control what record we are on. IF N EQ 1 THEN DO: statement would provide access to the first record in a data set.\nIn Proc Means procedure, in order to output the data set, we don’t write output or out statement on the first block, but we write it at the bottom after the VAR statement. As shown below. It is mind boggling, why SAS has this weird syntax and it is just not output = or out =. Based on current SAS syntax, we have to write output out = at the bottom. This is one of those strange inconsistency with SAS language, because other procedures like Transpose has out= at the top as shown in code below.\n\n\n\n\nFigure 5: Example of Proc Means Procedures Output Out= Syntax\n\n\n\nLastly, in SAS we use double asterisk (**) instead of caret (^) when we have to raise something to the power. SAS is probably one of the few language that still does this; others have adopted ^ or shift + 6. As shown below in formula asset_std.\n\n\n\n\nFigure 6: Example for using Double Asterisk (**)\n\n\nI’m still trying to figure out how to best balance my time, but I was definitely better at managing my time and taking good notes this week as compared to my first week. Hope I can keep at it!\nCheers to more learning and coding!!!\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@misc{shrestha2019,\n  author = {Mohit Shrestha},\n  title = {From Zero to 270 Lines of {SAS} Coding and Lessons Learned},\n  date = {2019-06-03},\n  url = {https://www.mohitshrestha.com.np/posts/2019-06-03-from-zero-to-270-lines-of-sas-coding-and-lessons-learned},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMohit Shrestha. 2019. “From Zero to 270 Lines of SAS Coding and\nLessons Learned.” June 3, 2019. https://www.mohitshrestha.com.np/posts/2019-06-03-from-zero-to-270-lines-of-sas-coding-and-lessons-learned."
  },
  {
    "objectID": "posts/2019-06-10-struggle-with-proc-sql-in-sas/index.html",
    "href": "posts/2019-06-10-struggle-with-proc-sql-in-sas/index.html",
    "title": "Struggle with PROC SQL in SAS",
    "section": "",
    "text": "Image source: https://i1.wp.com/www.quotes-compendium.com/images/quotes/Frederick_Douglass_1438470929.jpeg\n\n\nThis week had a good surprise. We learned about data wrangling in SAS using SQL language. I wasn’t expecting to learn SQL in the MSBA program as the program is primarily focused on SAS and R languages. So I was surprised and glad that we get to learn a bit of SQL as well, as it is the language of data science. Beside SAS, we need to know SQL to be successful in the real world, so having even a basic knowledge of SQL would be very helpful for me as I learn more and more of data analysis.\nThe main thing that we were learning was Create Table As Select (CTAS) in SAS with PROC SQL. It is supposed to be pretty standard DDL, creating table, adding columns, indexes, creating view, etc. But, I am getting nervous as I am struggling with it, and this is supposed to be the basics of SQL.\n\n\n\nFigure 1: Proc SQL\n\n\nI sought help from Helper SAS code file provided by the professor, and even consulted with one of my classmates. There were seven tasks to solve for this assignment, and I got stuck on #3 for a long time. It made me more nervous as I had a deadline to meet. I was writing the codes, but I wasn’t sure if I was doing it correctly. I wasn’t sure if the results were correct.\nSo I turned to Excel spreadsheet, something that I am very familiar with. Even though it is not the best practice to do what I was doing, I exported the smaller dataset created in SAS to Excel spreadsheet and used Excel’s Pivot tables, Vlookup. Index-Match, Pivot Chart to cross check the tables and graphs I was creating in SAS. I resorted to Excel to understand how the codes I wrote in SAS was working and cross-check the result output with Excel generated outputs. It is definitely not the most efficient way of learning, but I didn’t want to continue to get stuck. Using this technique, I was at least able to make good progress on the assignment for this week.\nI hope things get easier as I learn and practice more in the coming days. I know I am struggling, but I’d like to believe “If there is no struggle, there is no progress.”\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@misc{shrestha2019,\n  author = {Mohit Shrestha},\n  title = {Struggle with {PROC} {SQL} in {SAS}},\n  date = {2019-06-10},\n  url = {https://www.mohitshrestha.com.np/posts/2019-06-10-struggle-with-proc-sql-in-sas},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMohit Shrestha. 2019. “Struggle with PROC SQL in SAS.” June\n10, 2019. https://www.mohitshrestha.com.np/posts/2019-06-10-struggle-with-proc-sql-in-sas."
  },
  {
    "objectID": "posts/2019-06-17-macros-the-key-to-efficient-data-processing/index.html",
    "href": "posts/2019-06-17-macros-the-key-to-efficient-data-processing/index.html",
    "title": "Macros - the key to efficient data processing",
    "section": "",
    "text": "Image source: https://www.lynda.com/Excel-tutorials/Excel-2010-Macros/74463-2.html\n\n\nMy first introduction to Macro was in Microsoft Excel with VBA Macros. I haven’t used it extensively, but my last job required me to work with macros on few occasions. But, to be honest, I didn’t really liked working with macros then as most of the macros I worked with were created by someone else, without any instructions and proper descriptions of what the macros supposed to do. So, if you are not the one who wrote the macro, it is a bit more difficult to understand it. Unlike formulas, where it is very easy to backtrack the output; in macros, the output is a hard-coded number, so one has to look at the codes in detail to understand how the output was calculated. Therefore, I preferred working with formulas more than macros, however, I understand that working with huge datasets it is very efficient to create macros to automate tasks as it also reduces the possibility of human error that increases with many, repetitive keystrokes and tasks.\nLast week we learnt about using SQL (PROC SQL statements) in SAS language. It helped us to merge different sources of data, and process and analyze huge datasets. This week, we took a step further and learnt how to write Macros to i) automate processing huge chunks of the same code to analyze different components in the dataset, and ii) minimize the number of lines of codes for the data analysis. Macros are especially useful for automating and doing repeatable discrete tasks, where the analysis is an iterative process, and to pass data from step to step.\nHere are the key steps of writing any macro:\n\nIs it worth writing a macro vs lines of codes/formula?\n\nIf it is just few line of codes, then we can just repeat the codes, its a lesser hassle than creating macros.\nBut if the lines of code is 10 or more, and we have to repeat the same code multiple times for analyzing different variables, then it is worth writing a macro.\nIf we think we would use the code frequently in future, then it is worth writing a macro and saving for future use.\n\nOnce you decide to write a macro, start with writing the core part of the program. If it helps, break that program into multiple tasks. It is easier to write each block of code based on the order in which a user would perform this task if it were to be done manually.\nTake one component/field/variable that you want to analyze and make sure that the core lines of code runs smoothly without any error, and it does what it needs to do.\nOnce you have verified that the core lines of code are working, then you can create a macro as follows:\n\nTo start a macro:\n\n%MACRO macro-name <(parameter-list)></ option-1 <…option-n>>;\n\nPaste the clean code\n\nStart replacing the hard-coded input parameters that we defined earlier\n\nTo end a macro:\n\n%MEND\n\n\nWhen we run the chunk of code, it doesn’t return any output. We have to call the macro that we created with the defined input parameters.\nHere is an example:\n\nPROC FREQ DATA=NC_OPIOID_ANALYSIS nlevels; \nTABLE Gender / out=freq_out; \nRUN;\nNow writing macros for the above code, but making it dynamic to be used for analyzing other variables as well:\n%MACRO charProfile(inDat=, charField=); \nPROC FREQ DATA=&inDat nlevels; \nTABLE &charField / out=freq_out; \nRUN; \n%MEND;\nNow we call macro to analyze lets say field Medical School Name instead of Gender from NC_OPIOID_ANALYSIS table.\n%charProfile(inDat= NC_OPIOID_ANALYSIS, charField=Medical_school_name);\nIn this example, at first I wrote lines of core code to analyze Gender field only, but later when i created Macro named charProfile, I was able to reuse the same chunk of code to analyze 20 different character variables of that table, not just Gender. It was amazing to see how I was able to analyze all interested character variables from that datasets in such an efficient way using Macros.\nPreviously at work, I had to struggle understanding macros created by others without proper descriptions and instructions. So, my experience with macros were not pleasant; however from this weeks project, I got to learn about the key steps for writing a macro. I got to practice writing macros from the beginning and analyze the real world data set. Thus, it was much easier to follow and understand. I’ll definitely give writing Macros another chance, and given that I enjoyed this weeks assignment using macros, I think I use Macros more often to perform iterative tasks and analyze big data sets.\nCheers! to overcoming the fear of using Macros.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@misc{shrestha2019,\n  author = {Mohit Shrestha},\n  title = {Macros - the Key to Efficient Data Processing},\n  date = {2019-06-17},\n  url = {https://www.mohitshrestha.com.np/posts/2019-06-17-macros-the-key-to-efficient-data-processing},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMohit Shrestha. 2019. “Macros - the Key to Efficient Data\nProcessing.” June 17, 2019. https://www.mohitshrestha.com.np/posts/2019-06-17-macros-the-key-to-efficient-data-processing."
  },
  {
    "objectID": "posts/2019-06-23-a-beginning-to-unsupervised-machine-learning/index.html",
    "href": "posts/2019-06-23-a-beginning-to-unsupervised-machine-learning/index.html",
    "title": "A beginning to unsupervised machine learning",
    "section": "",
    "text": "Image source: https://i.pinimg.com/736x/09/50/d1/0950d1c1706b436bfa0f040e34133778.jpg\n\n\nLast week we learned about using macros in SAS. To be honest, in the beginning, I found it really difficult, but after reviewing it few times even after submitting my assignment, I now feel a bit more comfortable using it. But then again, classes are moving very fast; we’re jumping from one topic to another in a week. Learning a new concept every week is a bit overwhelming, especially when I am unable to keep up with the fast pace of the program. But I also find this exciting, It pushes me to challenge myself. It reminds me why I joined this program. I know it’s hard but I will keep at it.\nThis week we learnt about two powerful procedures of SAS – PROC UNIVARIATE and PROC FASTCLUS. So here are some things that are basic but I need to make a note of.\nPROC UNIVARIATE – If the PROC MEANS procedure does not produce the statistic needed for a data analysis then we can use PROC UNIVARIATE as it can do everything that PROC MEANS can do and much more. It provides a wider variety of statistics (than PROC MEANS) like moments, descriptive statistics, quantiles. In addition, it also generates graphs and will help us discover information about the distribution of data as well as identify extreme observations in our data.\nPROC FASTCLUS – It performs K-means cluster and other clustering analysis techniques. But we are going to focus on K-means cluster analysis on the basis of distances computed from one or more quantitative variables. I am more excited to learn about PROC Fastclus, as it starts our journey into unsupervised machine learning. Basically it means that we will be able to let the computers identify data on its own and use that identification to cluster similar pieces of data together. It picks up initial random observations called cluster seed, and by default it uses the Euclidean distance (distance between two points). It assumes a center for one of the cluster and finds the shortest distance between each of the observations to the center of the cluster, and finds how well the cluster represents the datasets. PROC Fastclus is designed to find good clusters, however, they may not be the best possible clusters. Moreover, the Fastclus procedure is intended for large data sets, with 100 or more observations.\nOne of the examples that we used in class for PROC FASTCLUS was to analyze types of beers, where we clustered all the beer data into 3 clusters – high alcohol low bitterness, medium alcohol medium bitterness, and low alcohol high bitterness. We were able to narrow down 2,410 numbers of different beers into 3 categories that sort of captured general variety of beers types in our data set. So, we can now just look into the particular categories that we are interested and find the few selection of beers that we are interested than looking at 2,410 different beers.\nUsing PROC Fastclus we can build a perfect model. We need to remember that clustering is not an exact science. We can build a perfect cluster by simply increasing the value of K or number of clusters. So, technically, we can have a cluster for each point of observation. But in practical it does not make any sense to do so. There is a trade off how well the data fits the cluster and how well we can define the cluster. Another thing to note is that if we are clustering more than 3 or 4 variables, then we have to really ask does it make sense or can we easily describe the lows and highs of each different things. It is hard to describe when we have 4 to 5 dimensions, it becomes much harder to explain and defend it. We have to be cautioned of clustering more than 10 clusters because we will end up with unintelligible clusters. Therefore, we need to have a fine balance of how many clusters we want so that we can look at the data and have enough clusters to analyze it better.\nAnother scenario that we looked at is where we separated a data set into 2 sets, one for training set and another set for test. For training set, we would run a model. Once we had a good model, we ran the test set data using that model to predict. I found it very fascinating to learn unsupervised learning where we are not trying to predict a target but we are trying to magically find hidden relationship within the data without trying to force those relationship by having something to try to predict. Key things for Unsupervised Learning:\n\nWe don’t have a label or target that we can go learn from.\nInstead we just look at natural grouping within the data or natural patterns that we find within the data.\n\nLastly, there is so much to learn. We just touched the basics of machine learning, but it is still very difficult. I often find myself overwhelmed with so much information to process. For this week’s assignment for the last task, i did get stuck and struggled a bit. So, I scheduled a one-on-one video session with my professor to go over some of the conceptual things that i was not clear about. This helped me immensely clear our my confusion and I was able to finish major chunk of the assignment. I still need to clean my codes and write additional comments to finish my assignment for this week, and finishing my assignments, I plan to go back and review the course materials again to firm my understanding of the material better.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@misc{shrestha2019,\n  author = {Mohit Shrestha},\n  title = {A Beginning to Unsupervised Machine Learning},\n  date = {2019-06-23},\n  url = {https://www.mohitshrestha.com.np/posts/2019-06-23-a-beginning-to-unsupervised-machine-learning},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMohit Shrestha. 2019. “A Beginning to Unsupervised Machine\nLearning.” June 23, 2019. https://www.mohitshrestha.com.np/posts/2019-06-23-a-beginning-to-unsupervised-machine-learning."
  },
  {
    "objectID": "posts/2019-07-01-building-multiple-linear-regression-model/index.html",
    "href": "posts/2019-07-01-building-multiple-linear-regression-model/index.html",
    "title": "Building Multiple Linear Regression Model",
    "section": "",
    "text": "Less of a Blog, More of a Note on Model Building\nThis week we studied about Estimation and Regression. For estimation, we specifically looked at T-Test or Student T-Test, and for regression, we particularly looked at linear regression.\nWe studied T-Test to evaluate differences in the means. In this process, I also came across a fun fact that T-test was developed by William Gosset. He introduced T-Test in 1908 while he worked at Guinness Brewery in Dublin. It was interesting to find out that he applied his findings using the T-Test to monitor the quality of stout in the production of dark beer. While he wanted to optimize and maintain the standard, he was researching how to make that happen for Guinness and that’s when he came up with T-Test statistics. At first, Guinness did not allow him to publish his research publicly in fear that other competitors might take advantage, and Guinness won’t have competitive advantage against them. But later Guinness agreed to allow Gosset to publish his finding under a pseudonym “Student”. That’s why, T-test is also known as Student T-test.\nT-test helps us compare means of two groups. There are two key assumptions for T-test:\n\nThe samples need to be independent\nThe samples need to be normally distributed (have a bell-curve or symmetry around the mean)\n\nWe usually use histogram to check if the distribution of the samples is normally distributed or not, but this week we also studied about a graph- scattered plot known as Q-Q plot, which helps us quickly eyeball the graph, where if the pattern of the scattered plot is an upward straight line, we can say that the samples are normally distributed.\nThe SAS PROC T-TEST procedure is used to test for the equality of means for a two-sample (independent group) t-test. The typical hypotheses for a two-sample t-test are:\nHo : µ𝟏 = µ𝟐\nHa : µ𝟏 ≠ µ𝟐\nOnce we run the PROC T-TEST, we get the results – pooled estimator and Satterthwaite value. If the variance is equal, then we look at the p-value of the pooled estimator, otherwise, if the variance is unequal, we look at the Satterthwaite, which is an alternative estimator. If the p-value is <0.05 (5%), we reject the null hypothesis that the means are equal, and we accept the alternative hypothesis that the means are different.\nAnother thing for T-test is that the class statement must consist of only two levels. It cannot be more than two levels. For example, we can run t-test between wheat beer and lager beer, but we cannot run t-test for 10 different types of beer at once.\nNow lets talk about linear regression modeling, it is a foundational tool for prediction, estimation and forecasting across a wide variety of domains and problems. In linear regression, there is a response variable (variable that we want to predict also known as y-variable, or dependent variable), and predictor variable (variable that we will use to explain the response variable, also known as x-variable, independent variable, or explanatory variable).\nIn statistics, we know that the linear equation has a straight line formula, y=a + bx, where a is the intercept which crosses y-axis, and b is the slope of that line. With this formula, we can determine the linear relationship between two variables. So regression is the statistical technique for finding the best fitted straight line through a set of data, and the resulting straight line is regression line, where the equation is also represented as y= a+bx. But, here, we also get an error, which is the difference between the actual data point (Y) and predicted data point (Y ̂) on the line. Total squared error is simply the sum of those differences.\nerror=Y-Y ̂\nTotal squared error: ∑\\[(Y-Y ̂)\\]\\^2\nWe need to know that the larger the correlation (which we can observe from the scattered plot), the less the error will be. So the accuracy of our linear regression depends on how well the points on the line correspond the actual data points, i.e., the amount of error. When there is a perfect correlation (r = 1 or r = -1), the linear equation fits the data perfectly. So we are looking for the r^2 value or adjusted r^2 value to be closer to 1 so that we can tell that the model is perfectly fitted. For example, if we have adjusted r^2 of 0.61 (61%), we can say 61% of the variance in the dependent variable is explained by the combination of independent variables of our model. We look at adjusted r^2 because it increases only if the new term improves the model more than it would be expected by chance.\nTalking about Multiple Linear Regression model, it has to have numeric variables, eg Categories, Nominal, Group etc. variables have to be One-Hot-Encoded which means all of those categories needs to be assigned to numeric variable for its identification to be able to use in the model. We also need to have complete records meaning observations containing nulls are ignored, and since we are dealing with linear regression, there needs to be linear relationship between the outcome variable and the independent variables.\n\n\n\n5 Key Assumptions of Regression\n\n\nAnother thing to note is how to build a multi linear model. There are few steps to take into consideration. At first we explore the data, where we use PROC means, univariate and sgplot to explore numeric and continuous variables, and generate descriptive statistics, and number of counts, number of missing value, mean, median, and discard any columns and observations that have a lot of missing values so that we can prepare or transform our data. Before that, we also run correlation between variables with PROC corr and plot the SGPLOT scatter, box, and histograms to determine the correlation between all the variables. Next step, we plot the distribution and see if it has skewed or normal distribution by looking at the histograms. If we find the variables to have skewed distribution then we use data step to transform those variables to have normal distribution by using LOG(x), LN(x), X^2, 1/X, SQRT(X). Among these whichever method gives us the normal distribution for those variable, we use that method in our model. Simultaneously, we also describe rules to identify and exclude outliers and extremes that affects our mean.\nWe also use PROC freq and sgplot to check the frequency to explore categorical / nominal variables. Then we look at preparing or transforming our continuous variable, basically replacing missing values with mean or median, and applying normalization model. We use that to prevent our results from being skewed because if we input skewed data, our model won’t predict good results. As we know Garbage In Garbage Out, we want to make sure to input good data to build and train our model, so it produces better results.\nWhen building our model we need to remember encoding categorical/nominal variables to 0/1s. We use PROC sql CASE WHEN or DATA step IF/THEN to do the encoding. Once we have transformed and prepared our dataset, we will partition our whole dataset into training set (75%) or test set (25%). We will iteratively train our model using training set. For evaluating the model, we check for the following steps after running PROC Reg:\n\n\n\nFigure 1\n\n\n\n\n\nFigure 2\n\n\n\n\n\nFigure 3\n\n\n\n\n\nFigure 4\n\n\n\n\n\nFigure 5\n\n\n\n\n\nFigure 6\n\n\nSo far, I have managed to understand PROC REG procedure to build multiple linear regression model. However, there are other alternatives to PROC REG procedure in building a regression model. These alternative procedures, I have yet to explore further. For reference, below are the snapshot of comparisons between the procedures provided in the class.\n\n\n\nFigure 7\n\n\n\n\n\nFigure 8\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@misc{shrestha2019,\n  author = {Mohit Shrestha},\n  title = {Building {Multiple} {Linear} {Regression} {Model}},\n  date = {2019-07-01},\n  url = {https://www.mohitshrestha.com.np/posts/2019-07-01-building-multiple-linear-regression-model},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMohit Shrestha. 2019. “Building Multiple Linear Regression\nModel.” July 1, 2019. https://www.mohitshrestha.com.np/posts/2019-07-01-building-multiple-linear-regression-model."
  },
  {
    "objectID": "posts/2019-07-08-lifes-a-marathon-not-a-sprint/index.html",
    "href": "posts/2019-07-08-lifes-a-marathon-not-a-sprint/index.html",
    "title": "Life’s a marathon, not a sprint",
    "section": "",
    "text": "A MINI BREAK TO RECHARGE MIND AND BODY FOR BETTER LEARNING.\n\n\n\nImage Source: Alamere falls\n\n\n\n“Keep close to Nature’s heart… and break clear away, once in awhile, and climb a mountain or spend a week in the woods. Wash your spirit clean.” ~ John Muir\n\nWeek 7 is the July 4th long weekend, so this week we didn’t have live session class for BAN 7001 (Probability and Statistical Modeling), and we only had office hour for BAN 7002 (Analytics Software Technology) class, where we quickly went over the mid-term project, to build a model to predict house prices using SAS. This mid-term project would require us to draw on all the SAS skills that we have acquired to predict home values for a major city in the U.S. This project will be a great exercise to practice all SAS concepts and practice that we have learned so far and use it to analyze a real world business scenario. In addition, with this project, we would wrap up everything we needed to learn about SAS programming language for this term. In such a short time of 7 weeks, we have covered a lot of topics in SAS programming and explored various analytical methods/tools used by Data Analyst/Data Scientist in examining various real world business scenarios. I believe I have acquired the building blocks to get started in SAS programming, and now I need to continue to practice more to master the language.\nNext week, we will start to learn R programming language, and I’m really excited about this one. I had been wanting to learn R or Python for a while. Both of these languages are open source and currently they are one of the most popular and widely used programming language for data analysis. Currently, there is an on-going discussion which language R or Python is better to learn in long run in data scientist communities. But I have read online, that once we learn one language, it is easier to learn and pick up other language as well. Therefore, I am excited to start learning R programming language starting next week. I am eager to find out about the advantages/disadvantages of using SAS vs R programming language for data analysis.\nThis week, during the office hour for BAN 7002, we just went over the material such as the basic construct of what we need to do for the house price prediction model project. After the office hour, I really haven’t had much time to work on the mid-term project yet due to prior commitment for July 4th long weekend. Also, since our submission deadline for this project was postponed to next week July 15, 2019 due to the July 4th long weekend, I have not rushed myself to finish the project yet.\nWe have been having back to back classes every week since the program started in May 22, 2019, and I decided to give myself a mini break this week. I took this time to relax and unwind as I believe it is important to pause, reflect and recharge so that I have the energy and focus required to continue my learning and do my best.\n\n“LIFE’S A MARATHON, NOT A SPRINT. PAUSING TO REFLECT IS NOT A PRIVILEGE — IT’S A MUST.” ~ DOMINIC SOH\n\nThe highlight of my July 4th weekend was a 8.5 mile hike to Alamere Falls in Point Reyes National Park, CA. Since I live in the Bay area, there’s plenty of natural reprieves within few miles from where I live. I along with my wife and friends decided to hike the Coast Trail from the Palomarin Trailhead near Bolinas to Alamere Falls. It was a beautiful hike that offered such a varied landscape. We walked through pine forests and lush green vegetation, saw beautiful wildflower blooms, lakeside views and amazing coastal panoramas. The main feature of the hike was the Alamere Falls which cascades over a 30 foot tall cliff and joins the Pacific ocean. It was a great hike, a definitely good break for myself, a good exercise for my body and mind.\nI haven’t made much progress this week as I took things slow. I feel like I took a much needed break to rejuvenate myself for more learning ahead. Meanwhile, I have managed to finish assignments for BAN 7001, and now I can focus on analyzing the data and building the predictive model using SAS for BAN 7002 mid-term project.\nAs for the BAN 7001, we are now studying about hypothesis testing, which kind of coincides nicely with us testing and evaluating our predictive models that we are building for BAN 7002 class. We will be defining a hypothesis and testing it against the alternative hypothesis in our model and testing how variables and the model predicts the house price.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@misc{shrestha2019,\n  author = {Mohit Shrestha},\n  title = {Life’s a Marathon, Not a Sprint},\n  date = {2019-07-08},\n  url = {https://www.mohitshrestha.com.np/posts/2019-07-08-lifes-a-marathon-not-a-sprint},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMohit Shrestha. 2019. “Life’s a Marathon, Not a Sprint.”\nJuly 8, 2019. https://www.mohitshrestha.com.np/posts/2019-07-08-lifes-a-marathon-not-a-sprint."
  },
  {
    "objectID": "posts/2019-07-15-wrapping-up-sas-and-beginning-r-programming-language/index.html",
    "href": "posts/2019-07-15-wrapping-up-sas-and-beginning-r-programming-language/index.html",
    "title": "Wrapping up SAS with mid-term project and kicking off R",
    "section": "",
    "text": "This is Week 8 of the MSBA program, an end to our learning of SAS programming language and beginning of R programming language. Our SAS learning journey ended with a mid-term project of analyzing data and building predictive multiple linear regression models using SAS.\n\n\n\nImage Source: https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSpAx15voLIn8JQb5-085gfIZ053alpvkFn1A&usqp=CAU\n\n\nFor this project, I had set up some goals for myself:\n\nUse everything I’ve learnt and practice more.\nWrite cleaner codes that are easier to read and follow.\nUse macro when possible to reduce repetitive lines of codes.\nBe efficient while coding – Rather than spending most of my time just on coding, I wanted focus more on understanding the presented dataset, and better analyze the data and results.\nBuild a robust narrative around it for a high-impact storytelling with data analysis.\n\nFor Data Exploration, I learnt new technique for finding correlation between variables to our single target variable AV_TOTAL. When I looked at SAS website for PROC CORR procedure for syntax and options available, I found out that using “BEST=Variable Number” option allows us to get the variables in descending order of magnitude of correlation coefficient. I found this super helpful as it avoided the need to eyeball the high correlation to the target variable using WITH statement (in my case it was with AV_TOTAL) and mention all numeric variables under VAR statement for Training dataset.\nUsing PROC CORR with BEST= options generated Pearson Correlation Coefficient table summary with highest correlated variables to AV_TOTAL in descending order with their respective values. This was something which was not discussed in class, I was glad to learn about it via online search as it is a very efficient quick way to do data analysis technique, so once when I learnt about it via online search, I shared it with my professor, and he thanked me for letting him know about this “Best=” option in PROC CORR procedures, as he plans to start using that option going forward in his analysis work.\n\n\n\nFigure 1: PROC CORR with BEST = options generated Pearson Correlation Coefficient table summary with highest correlated variables to target variable in descending order\n\n\nFor Model 1, I used the top 10 numeric variables that had high correlation that had high correlation using Proc Reg procedure\nFor Model 2, I used all numeric variables (not just the top 10 highly correlated variables) using Proc Reg with Selection-Forward Stentry = 0.10\nWe were required to create only two models, but I wanted to use Proc GLMSELECT as I had not used it before, so I created an additional model. So, for Model 3, I used Proc GLMSELECT.\nThe difference between Proc Reg and Proc GLMSELECT is that the later one allows us to be efficient by doing one hot encoding automatically for categorical/character variables that we specify in class statement.\n\nProc Reg can use only numeric variables to build linear regression model, so we would have to manually do one hot encoding character variables if we want to include them in our linear regression model.\nProc GLMSELECT can use both numeric and categorical/character variables to build linear regression models by listing character variables that we are interested in a class statement and include in model building equation. PROCGLM SELECT will do one hot encoding automatically for those listed categorical variables in class statement.\n\nIn model 3, I also used Lasso Selection method (because I had not used it before, and my Goal#1 was to “Use everything I’ve learnt and practice more.” Lasso basically casts a wide net for all variables mentioned and evaluates and builds the most compact model with less variables. However, in my case, Lasso did not give me the desired level of adjusted R square value and lower RMSE. So I played around using Selection method Forward with Slentry=0.10 in Proc GLMSELECT procedure, where I used all numeric and categorical/character variables to build the model and used class statement with categorical variables in Model 3.\nModel 3 gave me the best result.\n\n\n\nFigure 2: Results from Model 3\n\n\nNOTE: I had to remove some character variables from Proc GLMSELECT because if my training data set didn’t have records with character variables that are in predict and validate data set, the model will give me 0 value for prediction result. As there was no data in training data set to train the model so it will produce 0 as predicted value for the model.\nSince I spent most of my time with the mid-term project, I did not get a lot of time to go through the material provided for the R programming language. However, I am very excited to learn R programming language as it is one of the most popular free (open-source) statistical language along with Python, both of which has a massive community of users. More and more statisticians and data scientists are adopting R or Python for data analysis, as 1) it’s free, 2) the data analysis capabilities of both of these languages are endless, and 3) Existence of massive programming community for support.\nI had taken few R classes during my undergraduate, and back then we didn’t have R Studio. We used to write in R script in the old user interface, so with the introduction of R Studio, its much easier to learn R. It’s been a while since I last used R, and I’m really looking forward to learn more of it.\nThere are few differences between R and SAS. We can say goodbye to semicolons (;) at the end of the statements in R while we it is required in SAS. Another difference between R and SAS is that R is extremely case sensitive. For example, the “if” statement accepts a string regardless of whether it’s lower-case, upper-case or both in SAS, however, it always has be in lowercase in R. In R, there are more data types, but in SAS it’s either numeric (float, integer) or character.\nThese are some of the very basic things I noted on R during the class.There’s more to learn in R, but I need to go through the class presentation slides and videos. I’ll probably write more about it in my next blog. But, I’m very excited to learn R as it is very widely adopted, hopefully it will be a bit easier to learn than SAS because I’ve already taken couple of classes in my undergraduate, and our professor also said that it’s easier to learn another language if you have already learnt one.\nAfter doing the mid term project, I feel more confident about my SAS skills. It was an intense 8 weeks of learning, especially as I had not programmed in SAS before. But, it feels good to make it this far.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@misc{shrestha2019,\n  author = {Mohit Shrestha},\n  title = {Wrapping up {SAS} with Mid-Term Project and Kicking Off {R}},\n  date = {2019-07-15},\n  url = {https://www.mohitshrestha.com.np/posts/2019-07-15-wrapping-up-sas-and-beginning-r-programming-language},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMohit Shrestha. 2019. “Wrapping up SAS with Mid-Term Project and\nKicking Off R.” July 15, 2019. https://www.mohitshrestha.com.np/posts/2019-07-15-wrapping-up-sas-and-beginning-r-programming-language."
  },
  {
    "objectID": "posts/2019-07-22-basics-of-r-programming-language/index.html",
    "href": "posts/2019-07-22-basics-of-r-programming-language/index.html",
    "title": "Basics of R programming language",
    "section": "",
    "text": "Basic Operators and Functions\n\n\n\n\n\nLogical Operators\n\n\n\n\n\nConditional Logic\n\n\n\n\n\n\n\n\n\n\nMagrittr Pipes (%>%)\n\n\n\n\n\nImport CSV & XLS\n\n\n\n\n\nExport CSV & XLS\n\n\n\n\n\nPlotting Utils and GGplot2\n\n\n\n\n\nRMarkdown & Knitting\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@misc{shrestha2019,\n  author = {Mohit Shrestha},\n  title = {Basics of {R} Programming Language},\n  date = {2019-07-22},\n  url = {https://www.mohitshrestha.com.np/posts/2019-07-22-basics-of-r-programming-language},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMohit Shrestha. 2019. “Basics of R Programming Language.”\nJuly 22, 2019. https://www.mohitshrestha.com.np/posts/2019-07-22-basics-of-r-programming-language."
  },
  {
    "objectID": "posts/2019-07-22-the-journey-of-learning-r-programming-language-begins/index.html",
    "href": "posts/2019-07-22-the-journey-of-learning-r-programming-language-begins/index.html",
    "title": "The journey of learning R programming language begins",
    "section": "",
    "text": "Last week, we started R programming language, and to be honest, I’m very excited to be learning it. I had briefly written about it in my last blog post too.R is one of the most popular programming languages, besides Python, and the interest in the R language data analysis is soaring.\nI came across a blog post which compares R with SAS and Python. These are the most widely used languages in the field of analytics.\n\n\n\nImage Source: https://www.gangboard.com/blog/sas-vs-r-vs-python/\n\n\nRegardless of what language it is, the analytic process of every programming language is the same. So since we’ve already learned programming, learning R would be similar to SAS, so hopefully getting used to the syntax of R would be a bit easier as we’ve already learnt SAS. However, there are some key differences between R and SAS like instead of procedures PROCs in SAS, we will be learning Functions in R.Functions are similar to SAS macros but they are much more. Unlike SAS macros, Functions returns values. R is also a functional language where Functions are objects. In R everything is an object. Common object types are predefined, e.g. data objects, statistical functions, result objects etc. It is also possible to define new objects with object oriented programming techniques. R has been specifically designed as a language for statistical and mathematical problems, so it is a good example of a domain specific language.\nR consists of a small core program and a very elaborate package (e.g. R functions, data files, help files, code examples, PDF manuals, and unit tests). Because of the extensibility of R and availability of a very large number of packages, R is a very powerful programming language and thus, it saves a ton of time and improves the results. However, we need to make sure that the package that we are using is regularly updated. Since it is such a widely used language, there are many updates, so we just need to pay a bit more attention to the recent updates in the package when we use it.\nOur R learning will start with importing and exporting data, and then we will move on to how to structure the data. One of the big switch in R would be we won’t be using semicolons, and equals operator. We will be using <- operator to assign value to a variable. x<-5 statement would assign value 5 to variable x. Another big difference that I’ve noticed in R compared to SAS is the length of the codes – writing codes in R is usually much shorter than compared to SAS for the same task. As a result, we will spend more less to write the codes which will give us more time to analyse the results. Another thing that sets R apart from SAS is that R helps in producing outstanding graphs. With better graphs, analyzing the data becomes much easier, and in my case, it also makes me want to dive deeper into the analysis.\n\n\n\nImage Source: https://www.houseofbots.com/images/news/11673/cover.png\n\n\nThe chart above perfectly summarizes the key feature of R and for all the reasons above, I am looking forward to the intense 8 weeks of learning R programming language.\nNOTE: We need to install R first and then only install RStudio as R in order to take advantage of using the nice interface that RStudio provides.\nJust for future reference, I’ve also saved the slides from the class presentation that covers some basics of R in another blog post: https://www.mohitshrestha.com.np/blog/basics-of-r-programming-language\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@misc{shrestha2019,\n  author = {Mohit Shrestha},\n  title = {The Journey of Learning {R} Programming Language Begins},\n  date = {2019-07-22},\n  url = {https://www.mohitshrestha.com.np/posts/2019-07-22-the-journey-of-learning-r-programming-language-begins},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMohit Shrestha. 2019. “The Journey of Learning R Programming\nLanguage Begins.” July 22, 2019. https://www.mohitshrestha.com.np/posts/2019-07-22-the-journey-of-learning-r-programming-language-begins."
  },
  {
    "objectID": "posts/2019-07-29-data-wrangling-2-0-with-r/index.html",
    "href": "posts/2019-07-29-data-wrangling-2-0-with-r/index.html",
    "title": "Data Wrangling 2.0 with R",
    "section": "",
    "text": "We’ve been studying R for two weeks now, and so far we have studied the basics – analyzing the data with simple plotting- creating histogram, scatter plot.\nAnd now this week is data wrangling 2.0 with R, where we’ll be taking a step further into wrangling and look at data structuring where we will be using more techniques like joining, transposing to analyze the data better.\n\n\n\n\n\n\n\n\nFirst we looked at how to filter NAs, how to convert a character into a numeric data, convert date integer/character format into Date format using lubridate package, which is a R package that basically makes it easier to process any date related tasks.\nIn today’s blog, I’ll just make a note of some of these techniques that I thought would be most helpful.\n\n\nData wrangling with DPLYR\n\ninner join\nfull join\nbind rows and bind column\nunion all\nunion\n\n\n\nData wrangling with SQL\n\nSetDiff\n\n\n\nData wrangling with Tidy R\n\ngather\nspread\n\n\n\nJOINs statement in R are very similar to PROC SQL in SAS, but I found writing Join statement in R much easier and straight forward.\nThe most used technique of joining two datasets is using the function inner join which returns all observations in table_A and table_B, where the keys are equal, which is defined in the by statement. If we don’t specify the columns that we want to join by, by default, it will take the column names that match.\n\n\nInner Join\n\n\nFull join is another useful technique where it selects all matching and non-matching rows, and brings the records from all of the tables. I think this will be very useful when we are trying to find things that don’t match up.\n\n\nFull Join\n\n\nBut what happens when we don’t have matching columns? We just use DPLYR and specify a vector in the by statement:\nby = c(“x” = “x2”, “y” = “y2”)\nHere, x will be coming from Table 1, and x2 will be coming from Table 2. They don’t have to be the same name, but they have to be the same type.\n\n\nhttps://dplyr.tidyverse.org/reference/join.html\n\n\nThe other thing we learned is how to append rows together and bind columns by using functions bind_rows and bind_cols. They are pretty similar in terms of what they do. Bind rows just appends the tables together, and bind columns just puts the two tables together without the join. It may look like it joins the tables, but it’s not matching the keys. Also, in bind column, it just looks at the first column of first table. So in the example below, bind column just takes the observation data in COL_A of Table A.\n\n\nBind Rows and Bind Cols\n\n\nSo, the next function that is similar to bind_rows is union_all. Union also bind rows, but it doesn’t do sort, so we’ll just end up with a record once Table A and Table B are put together.\n\n\nUnion all\n\n\nThere’s another function union which is similar to union_all, where it appends the records with a sort distinct and returns the unique set of rows from Table A and B\n\n\nUnion\n\n\nOne of the other things that I think would be very helpful is SetDiff as it returns the rows that don’t match. There are times when we want to see what didn’t match, so in those cases SetDiff would be super helpful.\n\n\nSetdiff\n\n\nWe also learnt about data tidying, which is similar to transpose but a lot more powerful, where we have\n\ngather which allows us to take multiple columns and gathers them into key-value pairs and flips them into rows\nspread which takes two columns (key & value) and spreads in to multiple columns and makes rows into columns\n\n\n\nGather\n\n\n\n\nSpread\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@misc{shrestha2019,\n  author = {Mohit Shrestha},\n  title = {Data {Wrangling} 2.0 with {R}},\n  date = {2019-07-29},\n  url = {https://www.mohitshrestha.com.np/posts/2019-07-29-data-wrangling-2-0-with-r},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMohit Shrestha. 2019. “Data Wrangling 2.0 with R.” July 29,\n2019. https://www.mohitshrestha.com.np/posts/2019-07-29-data-wrangling-2-0-with-r."
  },
  {
    "objectID": "posts/2019-08-05-analyze-airlines-flight delays/index.html",
    "href": "posts/2019-08-05-analyze-airlines-flight delays/index.html",
    "title": "Analyze Airlines flight delays",
    "section": "",
    "text": "This week, we switched gears from basics of data wrangling process to build regression models using R programming language. Earlier, we learnt regression modeling using SAS programming language, and now we will do the same but using R. During the class exercise, we used R to rebuild our previous housing price prediction model which we coded in SAS. Both SAS and R regression predicted similar results for the given dataset. It was exciting to observe what we could do with SAS programming, we could do almost everything with R. It was pretty neat to see that R being an open-source programming language, it is as powerful tool as SAS, and in fact, R produced graphics/charts were better than SAS and needed much less coding. This exercise gave us confidence that we can code in both languages, and now I am curious to see whether I will find it easier to code and analyze data with SAS or R, and which programming language I will use more often as both SAS and R have their advantages and disadvantages.\nIn this week’s coding assignment, we had to analyze real data set of different airlines to figure out which flight path consistently was on-time, and which flight path suffered from frequent delays. Based on the actual flight data, we built a regression model to predict if the flight of certain airlines will be delayed or not. It was very interesting to see all the things that we found out. Since we used real data set, I can see our analysis being very useful when we need to choose an airport which can help us avoid a flight delay. This could also be helpful for airlines company to predict airline delays caused by various factors. Moreover, it could give individual airlines and airports an analysis of their performance thus helping them making a well-assessed decision.\n\n\n\nFlight Delays\n\n\nFor our assignment, we pulled the data on Airline On-Time Performance and Causes of Flight Delays from from Data.gov website for Department of Transportation, where we looked at 570,131 observations with 110 variables. From those observations, we looked were able to answer the following questions:\n1. How many flights experienced delays?\n\n2. What days of the week is the worst days for flight delays?\n\n3. Which destination airport experienced more delays?\n\n4. Top 10 flight path that had most delays\n\nBased on the data, we were able to find out that while roughly 82% of the flights were on time, almost 18% of the flights were delayed.\n\n\n\nFigure 1\n\n\nMonday (22.4%), Tuesday (19.17%) and Friday (18.71%) had the most number of flight delays.\n\n\n\nFigure 2a\n\n\n\n\n\nFigure 2b\n\n\nWe narrowed down the top 10 destination airports that experienced the most delays. Of the top ten, EWR (27%), PHX (26%), LGA (26%), BOS (25%), and SFO (23%) were the top five airports that had most delays.\n\n\n\nFigure 3\n\n\nBelow are the top 10 flight paths that had the most flight delays. FWA to ORD route was the top flight to experience most delays.\n\n\n\nFigure 4\n\n\n\nWorking with real data is an exciting experiment; I feel that it makes the process of learning and practicing much easier and practical. As excited as I am to add one more programming language to my skill set, regardless that coding is still a bit of a challenge to me, I have come to realize that this challenge is also driving my motivation. As Franklin D. Roosevelt once said “A smooth sea never made a skilled sailor”, I am up for this challenge. Moreover, converting big data into actionable insights, and being able to come up with business solutions is the main thing I wanted to learn from this MSBA program, and being able to do that in class assignments makes my learning journey much more fascinating and interesting.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@misc{shrestha2019,\n  author = {Mohit Shrestha},\n  title = {Analyze {Airlines} Flight Delays},\n  date = {2019-08-05},\n  url = {https://www.mohitshrestha.com.np/posts/2019-08-05-analyze-airlines-flight delays},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMohit Shrestha. 2019. “Analyze Airlines Flight Delays.”\nAugust 5, 2019. https://www.mohitshrestha.com.np/posts/2019-08-05-analyze-airlines-flight\ndelays."
  },
  {
    "objectID": "posts/2019-08-11-creating-custom-user-defined-functions-udf-and-k-means-clustering-in-r/index.html",
    "href": "posts/2019-08-11-creating-custom-user-defined-functions-udf-and-k-means-clustering-in-r/index.html",
    "title": "Creating Custom User Defined Functions (UDF) and K-Means Clustering in R",
    "section": "",
    "text": "Image source: https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2017/05/R-Functions-tutorial-1.jpg\n\n\nThis week we learned about writing our own custom functions in R. Lot of the packages that we use in R are functions created by other people, so learning to write our own functions is going to be really helpful in tackling specific problems to speed up the analyzing process.\nFor our last week’s assignment, since we didn’t know how to write our own functions, we had to repeat the same chunk of code again and again. But now, as we learned to write user-defined functions, we can now call the function anytime we need to analyze the data, enabling us to remove redundancy and duplication in code. The basic syntax of an R function definition is as follows:\nfunction_name <- function (arg_1, arg_2, ...) {\n    statements\n    return(object)\n    }\nUser-defined functions is going to be one of the most powerful tools we’d be learning in R. This will probably be the first step of writing our own function and deploying for other people to use. Going back to SAS, in SAS, we cannot write our own functions. There were macros, which is a bit similar to functions, but functions are much more powerful. In SAS, we had procedures that were handy. In R, we are required to create our own functions that mimics SAS’s PROC MEANS AND PROC FREQ functions. It was a good practice to recreate what we had previously done in SAS, but now we’d be using R to create similar analyzing task.\nAnother thing that we learned this class was clustering. We had learned how to create clustering in SAS, and now we’re learning about it in R. Clustering is the first step of unsupervised learning. So it helps in identifying the natural pattern in our dataset. So when we are creating predictive modeling, clustering will come very handy. We learned how to create clustering in R.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@misc{shrestha2019,\n  author = {Mohit Shrestha},\n  title = {Creating {Custom} {User} {Defined} {Functions} {(UDF)} and\n    {K-Means} {Clustering} in {R}},\n  date = {2019-08-11},\n  url = {https://www.mohitshrestha.com.np/posts/2019-08-11-creating-custom-user-defined-functions-udf-and-k-means-clustering-in-r},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMohit Shrestha. 2019. “Creating Custom User Defined Functions\n(UDF) and K-Means Clustering in R.” August 11, 2019. https://www.mohitshrestha.com.np/posts/2019-08-11-creating-custom-user-defined-functions-udf-and-k-means-clustering-in-r."
  },
  {
    "objectID": "posts/2019-08-19-text-analytics/index.html",
    "href": "posts/2019-08-19-text-analytics/index.html",
    "title": "Text Analytics",
    "section": "",
    "text": "This week we learned about text analytics. Text analytics is one of the most powerful tools in analytics field because of its ability to analyze unstructured data. Unstructured data are things like text (from a video, document, voice recording, email, tweets, e.t.c.). Given that 80% of all the data is in unstructured form, there is a massive potential in the analysis of unstructured data. Most of the unstructured data by itself is not valuable. It is very hard to interpret the message by just looking at the text data. So, that is why text analytics comes handy as we can analyze these unstructured data in a systematic way – identify and categorize topics of discussion. We can find what’s the most used words, the topics that has been discussed, basically find out the sentiment of people or of a document.\nFor text analytics, we follow the same analytical process that we have been doing throughout:\nStaging > Structuring > Cleansing > Summarize > Visualize > Model\nIn text analytics there are 4 main recipes:\nFor this, in R, there is a package called tidytext and TM package for text mining. In addition, there a is a good free book Text Mining with R. For text mining, the functions of interests are unnest_tokens which parses the text into words and ngrams. ngrams would take the two words together, which words together occur in common. get_stopword that will actually pull out frequently used stop words and make a list of those stop words. For example and, a, the and common words that are just junk words. If we look at the frequency of them they would just skew our results. Stopwords eliminates junk from our analysis. It actually applies to the stop words, get_sentiments. We have couple different sentiment lexicons that are available to us, like NRC, Bing, AFINN\nSo for this week’s assignment, we were given the task to analyze Trump tweets. We went to Trump Twitter Archive which basically archives all Trump tweets that can be downloaded in a .csv file. I downloaded all the tweets (37,125) from May 2009 to August 18, 2019 for my analysis."
  },
  {
    "objectID": "posts/2019-08-19-text-analytics/index.html#a.create-a-word-frequency-data-set-by-tokenize-the-tweet-text-remove-stop-words-filter-out-numbers-and-junk-and-finally-summarize-the-words-and-arrange-in-descending-order.",
    "href": "posts/2019-08-19-text-analytics/index.html#a.create-a-word-frequency-data-set-by-tokenize-the-tweet-text-remove-stop-words-filter-out-numbers-and-junk-and-finally-summarize-the-words-and-arrange-in-descending-order.",
    "title": "Text Analytics",
    "section": "a.Create a word frequency data set by tokenize the tweet text, remove stop words, filter out numbers and junk, and finally summarize the words and arrange in descending order.",
    "text": "a.Create a word frequency data set by tokenize the tweet text, remove stop words, filter out numbers and junk, and finally summarize the words and arrange in descending order.\n# --- parse and filter ------------------------------------------\ntweet_freq <- tweets %>%\n    unnest_tokens(word,text) %>%\n    anti_join(stop_words) %>%\n    filter(!str_detect(word,\"^\\\\d\")) %>% \n    filter(!word %in% c(\"t.co\",\"realdonaldtrump\",\"https\",\"http\", \"amp\", \"rt\",\"twitter\", \"p.m\", \"president\", \"people\", \"trump\"))"
  },
  {
    "objectID": "posts/2019-08-19-text-analytics/index.html#b.-join-the-word-frequency-data-to-the-sentiments-dataset-filter-out-positive-negatives",
    "href": "posts/2019-08-19-text-analytics/index.html#b.-join-the-word-frequency-data-to-the-sentiments-dataset-filter-out-positive-negatives",
    "title": "Text Analytics",
    "section": "b. Join the word frequency data to the sentiments dataset, filter out positive negatives",
    "text": "b. Join the word frequency data to the sentiments dataset, filter out positive negatives"
  },
  {
    "objectID": "posts/2019-08-19-text-analytics/index.html#c.-print-the-top-20-words-and-their-sentiment",
    "href": "posts/2019-08-19-text-analytics/index.html#c.-print-the-top-20-words-and-their-sentiment",
    "title": "Text Analytics",
    "section": "c. Print the top 20 words and their sentiment",
    "text": "c. Print the top 20 words and their sentiment\n# --- join to Bing lexicon ----------------------------------------\ntweet_sent <- tweet_freq %>%\n    inner_join(get_sentiments(\"bing\")) %>%\n    filter(sentiment %in% \n               c(\"positive\",\"negative\")) %>%\n    select(mnth, word, sentiment) %>%\n    group_by(word, sentiment) %>%\n    summarise(n=n()) %>%\n    arrange(sentiment,desc(n)) %>%\n    ungroup() %>%\n    top_n(20,n)"
  },
  {
    "objectID": "posts/2019-08-19-text-analytics/index.html#d.-create-a-bar-chart-of-top-20-words-color-by-sentiment",
    "href": "posts/2019-08-19-text-analytics/index.html#d.-create-a-bar-chart-of-top-20-words-color-by-sentiment",
    "title": "Text Analytics",
    "section": "d. Create a bar chart of top 20 words, color by sentiment",
    "text": "d. Create a bar chart of top 20 words, color by sentiment\n# — make a chart —\ntweet_sent %>%\n    ggplot(aes(reorder(word, n),n, fill=sentiment)) + \n    geom_col() +\n    labs(y=\"Positive & Negative Sentiment\",x= NULL) +\n    coord_flip()"
  },
  {
    "objectID": "posts/2019-08-19-text-analytics/index.html#e.-create-a-second-bar-charts-20-words-but-use-facet-wrap-to-create-two-charts.",
    "href": "posts/2019-08-19-text-analytics/index.html#e.-create-a-second-bar-charts-20-words-but-use-facet-wrap-to-create-two-charts.",
    "title": "Text Analytics",
    "section": "e. Create a second bar charts 20 words but use facet wrap to create two charts.",
    "text": "e. Create a second bar charts 20 words but use facet wrap to create two charts.\ntweet_sent %>%\n    ggplot(aes(reorder(word, n),n, fill=sentiment)) + \n    geom_col() + facet_wrap(~sentiment, scales=\"free_y\") +\n    labs(y=\"contrib to sentiment\",x= NULL) +\n    coord_flip()\n\n\n\nBar chart of top 20 words that Trump used along with the sentiment in color"
  },
  {
    "objectID": "posts/2019-08-19-text-analytics/index.html#convert-frequencies-to-a-sparse-matrix",
    "href": "posts/2019-08-19-text-analytics/index.html#convert-frequencies-to-a-sparse-matrix",
    "title": "Text Analytics",
    "section": "Convert frequencies to a sparse matrix",
    "text": "Convert frequencies to a sparse matrix\n## Convert frequencies to a sparse matrix\n\ntweet_dtm <-\n    tweet_freq %>%\n    cast_dtm(document, word, n)\ntweet_dtm\ntweettopic_model  <- LDA(tweet_dtm, k = 10, control = list(seed = 1234))\n\ntweet_topics <- tidy(tweettopic_model, matrix = \"beta\")\ntweet_topics\n\ntweet_top_terms <- \n    tweet_topics %>%\n    group_by(topic) %>%\n    top_n(5, beta) %>%\n    ungroup() %>%\n    arrange(topic, -beta)\n\nhead(tweet_top_terms)\n\ntweet_top_terms %>%\n    mutate(term = reorder(term, beta)) %>%\n    ggplot(aes(term, beta, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\") +\n    coord_flip()\n\n\n\nTop 10 Topics generated/identified with top 5 words used\n\n\nInstead of 10 topics, i wanted to narrow it down to 6 topics/issues that Trump used in his tweets on Twitter. From the bar chat below, we can categorize Trump tweets 6 topics: 1) China, 2) Vote, 3) Democrats, 4) Border and Wall 5) Tax, and 6) Fake News\n\n\n\n6 Topics generated/identified with top 5 words used\n\n\nThis was a very fun practice exercise assignment, where we got to play with real social media texts and analyzed its sentiments and learned to summarize the top topics that person or people are talking/discussing about."
  },
  {
    "objectID": "posts/2019-08-27-last-week-of-first-term/index.html",
    "href": "posts/2019-08-27-last-week-of-first-term/index.html",
    "title": "Last week of First term",
    "section": "",
    "text": "I always knew the MSBA program would be a bit of a challenge for me, given that I had no previous coding background. Also, the fast-paced learning experience hasn’t been that easy as well. These short three months of learning has been intense especially since I am back to school routine after a hiatus, and also because the online classes are optimized for maximum learning.\nThis is the final week for this term, and I’ve been swamped with a lot of projects – group project, probability and statistics project to find home equity loan, and two analytics projects with R – that I need to finish by next week. So far I’ve been working on the probability project, which I need to wrap up so that I can move on the two analytics projects.\nWithin three months, we tackled a lot of business scenario, we learnt a lot of techniques and procedures, and we took on numerous business problems from stock market analysis and wine quality analysis to predicting airlines delay and working with regression model on home equity loan. We tackled real world data problems by simultaneously learning and using various analytical tools. Now I feel that we have the basic foundation and recipes to analyze any business problems, softwares, tools and concept to solve those problems and come up with business solutions that are practical.\nSo far, it’s been a great learning journey. There were definitely times when I felt overwhelmed (like this week) with all the work that I had to finish. But as I realize all the new things that I am learning, I get very excited as well. So, cheers to more of learning ahead!!!!\n\n\n\nImage source: https://i0.wp.com/cmosshoptalk.com/wp-content/uploads/2020/04/Decorative-Lettering.png\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@misc{shrestha2019,\n  author = {Mohit Shrestha},\n  title = {Last Week of {First} Term},\n  date = {2019-08-27},\n  url = {https://www.mohitshrestha.com.np/posts/2019-08-27-last-week-of-first-term},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMohit Shrestha. 2019. “Last Week of First Term.” August 27,\n2019. https://www.mohitshrestha.com.np/posts/2019-08-27-last-week-of-first-term."
  },
  {
    "objectID": "posts/2019-09-01-final-project-of-term-1/index.html",
    "href": "posts/2019-09-01-final-project-of-term-1/index.html",
    "title": "Final project of Term 1",
    "section": "",
    "text": "My last week blog pretty much summed up my first term of the MSBA program at Wake Forest. This one will cover the last project of my first term.\nThe final project for both classes – BAN 7001 Probability and BAN 7002 Analytical – were similar looking at loan data set, but the objectives of the regression model and the datasets were different for each class.\nFor Probability class, each student had to develop a predictive regression model to predict the amount of loan that the customer of the bank would request. We had data from the bank, so based on different characteristics of the customers, we would build the model and come up with a prediction model estimating the amount of loan the client would request. For this class, I used SAS to build the prediction model.\nFor Analytical class, I looked at a different financial institution’s loan dataset and used R to analyze the data, instead of SAS. I followed all the procedures I had been learning and using throughout this term – exploring, removing outliers or extreme values, creating graphs and correlation plots, deriving new variables – that would help us predict and come up with two regression models. I then had to compare between the two and determine which model would give better prediction.\nUnlike the Probability class where the objective of the project was to predict the amount of loan, for this class, I had to identify which loan request would be defaulted. For Model 1, I just had five predictors, but for Model 2 I built a larger model and let the model decide which variable to pick using step wise method. Once I had the final model, I compared the accuracy, AIC value, confusion matrix, accuracy, area under the curve, and picked the model which had the best prediction capability of identifying which loan request would probably result into default.\n\n\n\nFigure 1: Correlation Matrix\n\n\n\n\n\nFigure 2: Identified Clusters\n\n\nFor this, I also broke down the dataset into training and test dataset. I built the model using training dataset and used test dataset to validate both Model 1 and Model 2. Once both test and training dataset gave same result, I confirmed which model had better predictability, and it was Model 2 in my case.\n\n\n\nFigure 3: Model Performance Comparsion\n\n\nI enjoyed working on my final project especially since we had to use both SAS and R for similar datasets. It took me a lot longer to finish the project than I had initially anticipated, but the process of figuring out things to get to the result was very interesting and challenging.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@misc{shrestha2019,\n  author = {Mohit Shrestha},\n  title = {Final Project of {Term} 1},\n  date = {2019-09-01},\n  url = {https://www.mohitshrestha.com.np/posts/2019-09-01-final-project-of-term-1},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMohit Shrestha. 2019. “Final Project of Term 1.” September\n1, 2019. https://www.mohitshrestha.com.np/posts/2019-09-01-final-project-of-term-1."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Mohit Shrestha",
    "section": "",
    "text": "Want to support my blog?  Or if you’re interested in working with me, I’m open to freelance work. You can book an appointment with me on Calendly.\n\n\n\n\n\n\n\n\n\n\n    \n      \n      \n\n\n\n\n\n \n\n\n\n\nFinal project of Term 1\n\n\n\n\n\n\n\nMSBA\n\n\nDataScience\n\n\nData Analytics\n\n\nR\n\n\nClustering\n\n\nPredictive\n\n\nRegression\n\n\nModels\n\n\n\n\nThe final project for both classes – BAN 7001 Probability and BAN 7002 Analytical – were similar looking at loan data set, but the objectives of the regression model and the…\n\n\n\n\n\n\nSep 1, 2019\n\n\nMohit Shrestha\n\n\n\n\n\n\n \n\n\n\n\nLast week of First term\n\n\n\n\n\n\n\nMSBA\n\n\nDataScience\n\n\nData Analytics\n\n\nR\n\n\n\n\nWithin three months, we tackled a lot of business scenario, we learnt a lot of techniques and procedures, and we took on numerous business problems from stock market…\n\n\n\n\n\n\nAug 27, 2019\n\n\nMohit Shrestha\n\n\n\n\n\n\n \n\n\n\n\nText Analytics\n\n\n\n\n\n\n\nMSBA\n\n\nDataScience\n\n\nData Analytics\n\n\nR\n\n\nText Analytics\n\n\nNatural Language Processing\n\n\nNLP\n\n\n\n\n“Text Analytics, also known as text mining, is the process of examining large collections of written resources to generate new information, and to transform the unstructured…\n\n\n\n\n\n\nAug 19, 2019\n\n\nMohit Shrestha\n\n\n\n\n\n\n \n\n\n\n\nCreating Custom User Defined Functions (UDF) and K-Means Clustering in R\n\n\n\n\n\n\n\nMSBA\n\n\nDataScience\n\n\nData Analytics\n\n\nR\n\n\nCustom Functions\n\n\nUser Defined Functions\n\n\nK-Means\n\n\nClustering\n\n\n\n\nWe learned to write user-defined functions, we can now call the function anytime we need to analyze the data, enabling us to remove redundancy and duplication in code.…\n\n\n\n\n\n\nAug 11, 2019\n\n\nMohit Shrestha\n\n\n\n\n\n\n \n\n\n\n\nAnalyze Airlines flight delays\n\n\n\n\n\n\n\nMSBA\n\n\nDataScience\n\n\nData Analytics\n\n\nR\n\n\n\n\nIn this week’s coding assignment, we had to analyze real data set of different airlines to figure out which flight path consistently was on-time, and which flight path…\n\n\n\n\n\n\nAug 5, 2019\n\n\nMohit Shrestha\n\n\n\n\n\n\n \n\n\n\n\nData Wrangling 2.0 with R\n\n\n\n\n\n\n\nMSBA\n\n\nDataScience\n\n\nData Analytics\n\n\nR\n\n\nData Wrangling\n\n\nDplyr\n\n\n\n\nNow this week is data wrangling 2.0 with R, where we’ll be taking a step further into wrangling and look at data structuring where we will be using more techniques like…\n\n\n\n\n\n\nJul 29, 2019\n\n\nMohit Shrestha\n\n\n\n\n\n\n \n\n\n\n\nBasics of R programming language\n\n\n\n\n\n\n\nMSBA\n\n\nDataScience\n\n\nData Analytics\n\n\nR\n\n\nBasics of R programming language\n\n\n\n\nBasic Operators and functions, Logical Operators, Conditional Logic, Loops, For Loop, While Loop, Magrittr Pipes, Pipes, Import & Export, Plotting, RMarkdown.\n\n\n\n\n\n\nJul 22, 2019\n\n\nMohit Shrestha\n\n\n\n\n\n\n \n\n\n\n\nThe journey of learning R programming language begins\n\n\n\n\n\n\n\nMSBA\n\n\nDataScience\n\n\nData Analytics\n\n\nR\n\n\nSAS vs R vs Python\n\n\nSAS\n\n\nPython\n\n\nWhy learn R\n\n\nProgramming languages\n\n\n\n\nRegardless of what language it is, the analytic process of every programming language is the same. So since we’ve already learned programming, learning R would be similar to…\n\n\n\n\n\n\nJul 22, 2019\n\n\nMohit Shrestha\n\n\n\n\n\n\n \n\n\n\n\nWrapping up SAS with mid-term project and kicking off R\n\n\n\n\n\n\n\nMSBA\n\n\nDataScience\n\n\nData Analytics\n\n\nSAS\n\n\nR\n\n\nCoding\n\n\nPredictive\n\n\nMultiple Linear Regression\n\n\nRegression\n\n\n\n\nThis is Week 8 of the MSBA program, an end to our learning of SAS programming language and beginning of R programming language. Our SAS learning journey ended with a…\n\n\n\n\n\n\nJul 15, 2019\n\n\nMohit Shrestha\n\n\n\n\n\n\n \n\n\n\n\nLife’s a marathon, not a sprint\n\n\n\n\n\n\n\nMSBA\n\n\nDataScience\n\n\nData Analytics\n\n\nSAS\n\n\nCoding\n\n\n\n\nA mini break to recharge mind and body for better learning. “Keep close to Nature’s heart… and break clear away, once in awhile, and climb a mountain or spend a week in…\n\n\n\n\n\n\nJul 8, 2019\n\n\nMohit Shrestha\n\n\n\n\n\n\n \n\n\n\n\nBuilding Multiple Linear Regression Model\n\n\n\n\n\n\n\nMSBA\n\n\nDataScience\n\n\nData Analytics\n\n\nSAS\n\n\nCoding\n\n\nRegression\n\n\nMultiple Linear Regression\n\n\nModels\n\n\nStudent T-Test\n\n\n\n\nThis week we studied about Estimation and Regression. For estimation, we specifically looked at T-Test or Student T-Test, and for regression, we particularly looked at…\n\n\n\n\n\n\nJul 1, 2019\n\n\nMohit Shrestha\n\n\n\n\n\n\n \n\n\n\n\nA beginning to unsupervised machine learning\n\n\n\n\n\n\n\nMSBA\n\n\nDataScience\n\n\nData Analytics\n\n\nSAS\n\n\nCoding\n\n\nUnsupervised Machine Learning\n\n\nML\n\n\n\n\nLast week we learned about using macros in SAS. To be honest, in the beginning, I found it really difficult, but after reviewing it few times even after submitting my…\n\n\n\n\n\n\nJun 23, 2019\n\n\nMohit Shrestha\n\n\n\n\n\n\n \n\n\n\n\nMacros - the key to efficient data processing\n\n\n\n\n\n\n\nMSBA\n\n\nDataScience\n\n\nData Analytics\n\n\nSAS\n\n\nCoding\n\n\nMacros\n\n\n\n\nMy first introduction to Macro was in Microsoft Excel with VBA Macros. I haven’t used it extensively, but my last job required me to work with macros on few occasions.\n\n\n\n\n\n\nJun 17, 2019\n\n\nMohit Shrestha\n\n\n\n\n\n\n \n\n\n\n\nStruggle with PROC SQL in SAS\n\n\n\n\n\n\n\nMSBA\n\n\nDataScience\n\n\nData Analytics\n\n\nSAS\n\n\nCoding\n\n\n\n\nThis week had a good surprise. We learned about data wrangling in SAS using SQL language. I wasn’t expecting to learn SQL in the MSBA program as the program is primarily…\n\n\n\n\n\n\nJun 10, 2019\n\n\nMohit Shrestha\n\n\n\n\n\n\n \n\n\n\n\nFrom zero to 270 lines of SAS coding and lessons learned\n\n\n\n\n\n\n\nMSBA\n\n\nDataScience\n\n\nData Analytics\n\n\nSAS\n\n\nCoding\n\n\n\n\nIt’s my second week of learning to program in SAS. This week, we were introduced to the process of Data Wrangling. Data Wrangling is the process of acquiring, shaping…\n\n\n\n\n\n\nJun 3, 2019\n\n\nMohit Shrestha\n\n\n\n\n\n\n \n\n\n\n\nEverybody has to start somewhere\n\n\n\n\n\n\n\nMSBA\n\n\nDataScience\n\n\nData Analytics\n\n\n\n\nEverybody has to start somewhere. You have your whole future ahead of you. Perfection doesn’t happen right away - Haruki Murakami\n\n\n\n\n\n\nMay 29, 2019\n\n\nMohit Shrestha\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "project_portfolio/2020-05-18-d4wn-project/index.html",
    "href": "project_portfolio/2020-05-18-d4wn-project/index.html",
    "title": "D4WN Project: Analysis by Mohit Shrestha",
    "section": "",
    "text": "Client: data.org (a collaboration between The Rockefeller Foundation and MasterCard’s Center for Inclusive Growth)\nSummary: “Use of Business Intelligence for Informal Workers” is one of just eight winning ideas out of over 1200 submissions to the data.org Inclusive Growth and Recovery Challenge. An estimated 90% of workers in Mozambique are within the informal labor market, providing them little to no safety nets, financial security, or career potential.\n\nData for Workforce Nurturing (D4WN) is a project co-created and implemented by a multinational consortium led by Fundación Capital (Latin America - Mozambique), UX Information Technologies (Mozambique), and Data Elevates (United States) to empower workers by sharing informal labor market insights and personal advice derived from data generated by two local digital platforms, Biscate and Com-Hector. The project is being funded by data.org, a platform for partnerships to build the field of data science for social impact. We believe that data science can help generate actionable insights for workers, data narratives for stakeholders to engage in data driven decision making for labor policies, and improve the design and development of digital platforms that can service informal workers and improve their financial outcomes.\nIn this project, Data Elevates leads the data analytics approach and implementation to generate and utilizes real-time, data-driven insights from the informal labor market.\nAnticipated Impact: To improve employability and career opportunities for informal workers across Mozambique.\n\n\n\n\n\nRelevant website links:\n\nD4WN Official Website\nProject’s Analysis Github Repository\n\nAnalysis:\n\nGenerate table for Valid D4WN all activities\nRetain Accounts Prediction (Machine Learning)\n\n\n\n\nCitationBibTeX citation:@misc{2020,\n  author = {},\n  title = {D4WN {Project:} {Analysis} by {Mohit} {Shrestha}},\n  date = {2020-05-18},\n  url = {https://www.mohitshrestha.com.np/project_portfolio/2020-05-18-d4wn-project},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“D4WN Project: Analysis by Mohit Shrestha.” 2020. May 18,\n2020. https://www.mohitshrestha.com.np/project_portfolio/2020-05-18-d4wn-project."
  },
  {
    "objectID": "project_portfolio.html",
    "href": "project_portfolio.html",
    "title": "Project Portfolio",
    "section": "",
    "text": "Want to support my work?  Or if you’re interested in working with me, I’m open to freelance work. You can book an appointment with me on Calendly.\n\n\n\n\n\n\n\n\n\n\n    \n      \n      \n\n\n\n\n\n\n\n\n\n\nD4WN Project: Analysis by Mohit Shrestha\n\n\n\nDataScience\n\n\nData Analytics\n\n\nDataElevates\n\n\nD4WN\n\n\n\n“Use of Business Intelligence for Informal Workers” is one of just eight winning ideas out of over 1200 submissions to the data.org Inclusive Growth and Recovery Challenge.…\n\n\n\n\n\n\nMay 18, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "recommendations.html",
    "href": "recommendations.html",
    "title": "Recommendations",
    "section": "",
    "text": "Vice President at ICF\n\nMohit has an exceptional ability to think through larger problems and break them down into the component pieces needed to consider the whole, while not loosing track of the larger problem. His genuine interest in problem solving is clear in everything he does, as is his creative and analytical skills. He has an unquestionable work ethic and a great ability to manage his time to get everything done in a timely manner. Mohit is also an excellent person manager, taking the time needed to properly train staff as well as to understand their interests and help them grow in their positions.\n\n\n\n\n\n\n\n\n\n\n\n\n\nHead of Market & Customer Intelligence at Depsys\n\nMohit’s work ethic is impressive. He is always detail-oriented and willing to put in time and effort to assure timely project deliveries. In addition to being extremely organized with a large amount of data, Mohit also possesses the capability to step back and ask questions about the big picture. He also actively seeks opportunities to improve workflow efficiency, which benefits the wider team as well. Working with Mohit is a great learning experience for team members of varying levels of experience.\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolicy Advisor at Form Energy\n\nMohit is a fantastic mentor and masterful analyst. He has a unique ability to develop analytical tools, explain them clearly, and keep client needs central to their design. Mohit’s commitment and passion for our work bred enthusiasm across the entire team.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSenior Engineer at Exelon\n\nMohit has an amazing work ethic. His commitment to ensuring projects/tasks were both accurate and completed on time made him an invaluable asset to our group. Mohit’s love of data analysis and eagerness for learning new analytical approaches meant that he was always improving work-efficiency. In addition, Mohit was always willing to manage and train junior members of the group. I thoroughly enjoyed working with such a motivated and enthusiastic individual.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEconomist at US Environmental Protection Agency (EPA)\n\nIn addition to being a stellar analyst with superb attention to detail and a great work ethic, in the two years we worked together at ICF, Mohit really impressed me with his commitment to helping others in the team. Always willing to stay late to help a colleague learn how to operate a model or explain a concept to a new hire, Mohit was directly responsible for creating a great team environment and helping onboard others and increase their productivity. Extremely organized and always professional, Mohit was able to navigate challenging deadlines and high pressure situations while remaining calm and consistently delivering high quality work products. Being a major contributor to client deliverables, his strong grasp of the material meant he was always able to clearly and effectively communicate findings internally and externally, making him a vital component of the project teams we were both part of. He has been a genuine pleasure to work with and he would be a wonderful asset to any team in the future.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSenior Market Fundamentals Analyst at Constellation\n\nI had the pleasure of interacting with and learning from Mohit at ICF. Mohit is highly analytical, sincere, and hard-working. He is also extremely patient in dealing with questions and has an eye for detail. In addition to these qualities, Mohit is also an attentive listener which helps him tailor solutions to various stakeholder needs.\n\n\n\n\n\n\n\n\n\n\n\n\n\nConsultant at Energy and Environmental Economics\n\nMohit was a great consultant to work with and a great mentor to learn from. He knows all the details about the analysis work and was able to articulate the insights in a very neat way. I enjoyed working with Mohit!\n\n\n\n\n\n\n\n\n\n\n\n\n\nElliott Professor of Economics and Business at Hampden-Sydney College\n\nMohit Shrestha is one of the finest analytical thinkers/modelers that I have worked with in the last 25 years. He and I did many projects together, from modeling and forecasting US economic activity to doing regional analysis for a variety of US and international projects. Mohit is adept in R, SAS, EViews, IMPLAN (Impact Analysis), and many other statistical and data analysis environments. He is a superb writer and speaker. There are comparatively few people I would put on my research team, but Mohit Shrestha is certainly one of them."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "resources",
    "section": "",
    "text": "free icons from fontawesome + the {fontawesome} R package for infusing your slides, website, RMarkdowns/Quarto Markdowns, and more with an extra splash of fun"
  },
  {
    "objectID": "resources.html#data-viz",
    "href": "resources.html#data-viz",
    "title": "resources",
    "section": "\nBar Chart\n data viz",
    "text": "Bar Chart\n data viz\n\n\nfrom Data to Viz, a choose-your-adventure-type experience for creating your best data visualizations"
  },
  {
    "objectID": "resources.html#reporting-presentations",
    "href": "resources.html#reporting-presentations",
    "title": "resources",
    "section": "\nDesktop\n reporting & presentations",
    "text": "Desktop\n reporting & presentations\n\n\nQuarto supports a variety of formats for creating presentations, including:\n\nrevealjs (HTML)\nPowerPoint (MS Office)\nBeamer (LaTex/PDF)\n\n\nMeet xaringan: Making slides in R Markdown, by Allison Hill"
  },
  {
    "objectID": "resources.html#learning-materials",
    "href": "resources.html#learning-materials",
    "title": "resources",
    "section": "\nBook Open\n learning materials",
    "text": "Book Open\n learning materials\n\nEver feel like you need a pick-me-up while learning stats or a new R tool? Look no further than Allison Horst’s Data Science Illustrations, featuring the cutest and most educational fuzzy monsters you’ll ever see!\nMastering Shiny, by Hadley Wickham\nHappy Git and GitHub for the useR, by Jenny Bryan\nR for Data Science, by Hadley Wickham\nW3Schools, particularly for their HTML & CSS tutorials"
  },
  {
    "objectID": "resources.html#data-science-communities",
    "href": "resources.html#data-science-communities",
    "title": "resources",
    "section": "\nUsers\n data science communities",
    "text": "Users\n data science communities"
  },
  {
    "objectID": "resources.html#web-accessibility",
    "href": "resources.html#web-accessibility",
    "title": "resources",
    "section": "\nWindow Maximize\n web accessibility",
    "text": "Window Maximize\n web accessibility\n\nThe A11Y Project is a community-driven effort to make digital content more accessible. The A11Y Project not only has a ton of great blog posts, but also WCAG compliance checklist.\nCoblis, a color blindness simulator – just upload a file and view your color palettes as they would appear under different types of color deficiencies"
  },
  {
    "objectID": "resources.html#misc.",
    "href": "resources.html#misc.",
    "title": "resources",
    "section": "\nFace Smile\n misc.",
    "text": "Face Smile\n misc.\n\nDiscover new music to jam out to while you hack at Every Noise at Once (HT: Jeanette Clark)"
  },
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Mohit Shrestha",
    "section": "",
    "text": "Services\n\n\n\nDATA ANALYTICS\nExpert at data wrangling, ETL, and latest analytical tools to conduct through quantitative analysis.\n\n\nVISUALIZATIONS\nDesign and develop compelling data visualizations via a variety of techniques such as Tableau, R, Excel.\n\n\nMACHINE LEARNING\nResearch and implement appropriate Machine Learning algorithms and tools to discover solutions hidden in large data sets.\n\n\n\n\n\n\n\n\n\n\n\nBUSINESS INTELLIGENCE\nTranslate data into actionable business insights to drive business values and find business solutions.\n\n\nMARKET RESEARCH\nGather and organize information on target markets uncovering key insights.\n\n\nSTORYTELLER/ COMMUNICATOR\nMeasure, visualize, and communicate complex analytics to technical and nontechnical audiences with clarity, precision, and influence through a compelling narrative.\n\n\n\n\n\n\n\nPhone\n+1 (443) 823-4701\n\n\nEmail\ncontact@mohitshrestha.com.np\n\n\n\n\nInterested in my services? Hire me!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Driven Problem Solver\nEnergy Market Expert"
  }
]